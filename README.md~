Insight Data Engineering - Coding Challenge
===========================================================
Jon Neff
jon.m.neff@gmail.com

DEPENDENCIES:

words_tweeted.py requires Spark.  

Make sure you have Java 6 or later installed.  

Go to 
https://spark.apache.org/downloads.html

and download Spark release 1.4.0, Pre-built for Hadoop 2.6 and later.

Copy run.sh as well as src and tweet_input to the Spark install directory, i.e.,

<path from root>/spark-1.4.0-bin-hadoop2.6

Then use run.sh to generate output.  

NOTES:

Feature 1:  Words Tweeted

I chose a MapReduce approach using Spark since word counting is a classic 
problem for MapReduce. Spark is inherently scalable and by using
in-memory processing it typically runs much faster than Hadoop MR jobs.  While
I realize that this implementation will be slow for relatively small files on
a single node, this approach can easily be transferred to a cluster and scale
to very large inputs.  

It's a bit like swatting a fly with a bulldozer but it will scale.  

I considered implementing a simple MapReduce using the Python multiprocessing
package similar to the example given here:

https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/

I decided to stick with Spark since I know it better than the multiprocessing
package.  

Feature 2:  Median Unique

Calculating a running median is not a trivial problem.  The naive approach is to 
just create a list containing number of unique words for each tweet and add to 
the list as new tweets come in.  As each new tweet arrives, you sort the list 
and pick the median.  No problem when the list is small.  The problem comes when
the list gets very large and you have to sort with each incoming tweet. THE
REQUIREMENT TO UPDATE THE MEDIAN WITH EACH INCOMING TWEET IS THE DRIVING 
REQUIREMENT.  If this requirement could be relaxed, or you would be satisfied
with an approximate median, the problem would be much easier.  

I considered several approaches including the following:
Binning:  http://www.stat.cmu.edu/~ryantibs/median/

QuickSelect, blist tree structure, skiplist:  
https://rhettinger.wordpress.com/tag/running-median/

I finally settled on a priority queue two-heap approach as described here
http://stackoverflow.com/questions/10657503/find-running-median-from-a-stream-of-integers

and here:
http://www.ardendertat.com/2011/11/03/programming-interview-questions-13-median-of-integer-stream/

I used the two-heap approach because it has O(logN) time complexity and I found
an implementation that was short and easy to understand.  (Computing the median
is only O(1) but insertion is O(logN).)

I am not satisfied with this approach because the space complexity will
eventually kill it.  In real life I would push back on the requirement to see 
if a sampling approach to find an approximate median or a moving window median
would be sufficient.  Otherwise the heap will eventually run out of memory.  

SOME NOTES ON SIZE OF THE PROBLEM:

Twitter says they have 500M tweets per day.  That’s 15000M tweets per month, or 
1.5e10 tweets per month.  Each tweet is at most 7 bits.  

How big does the data structure need to be in order to 
account for all the words that might occur in a month’s worth of tweets?  
Oxford English Dictionary says that about 50,000 words account for about 95% 
of the words in its corpus.  But to get the tail at 99%, you would need a 
million words.  

# tweets/day	5.00E+08	500 million tweets per day		
bits per tweet	7	Avg num unique words only		
days/month	30			
bits/month	1.05E+11			
GB/bit	1.25E-10			
GB	1.31E+01	1.3 million gigabytes		The heap won't fit on one machine.


